\section{Reviewer Perspective}
%What is the strength of the paper? What are its weaknesses?
\subsection{Research Question}
The paper at hand places in a very interesting and possibly disruptive research field focussing on uniquely human intelligence. This places the research presented in this paper amidst the many discussions whether AI can learn skills beyond pattern recognition and solving factual problems, a question that is currently unanswered with existing results being controversial \cite{critics1}. In this context, Kosinski's work is a well-structured presentation of arguments for LLMs having ToM such as previous cases of autonomous emergence of skills in AI and language being a solid candidate for conveying information about ToM. Another point to be applauded is the consideration of ToM-tests already being part of a model's training data, making adequate testing for ToM difficult. The presented methods contained lots of thoughts on robustness of the findings such as switching correct and wrong answers and testing the models using scrambled tests and adjusting prompts to be non-suggestive. All this shows a great understanding of the implications when applying methods from human psychology to AI.

\subsection{Methods and Results}
However, the resulting tests lack statistical relevancy due to their small number and limited scope. With only 40 tests per model and only testing for false beliefs excluding tests for e.g. emotional or social intelligence the presented research can be merely seen as a first step into applying ToM tests designed for humans to AI. The author could have made use of existing ToM-test databases such as SocialIQa \cite{socialiqa}. Shapira et al. provide an extensive list of such datasets in their work containing roughly 2,900 tests for various facets of ToM published from 2016 to 2021 and thus could have been used in Kosinski's paper published in 2023 \cite{critics1}.
 
One might argue that using an existing database defeats the purpose of rewriting false-belief tests in order to eliminate the possibility of overlapping training and test data. While this seems to be a strong argument, the author does not test his hypothesis by e.g., testing LLMs using newly written false-belief tests and pre-existing ones or the pre-existing datasets referred to in the previous section. This is accompanied by the criticism voiced by Marcus and Davis \cite{critics2} who suggest that since the false-belief tasked used as a basis for Kosinski's dataset are well-established, highly referenced and mentioned in lots of other literature, it is very likely that GPT-3 would be able to recognise the tests even in their altered form since the structure of the test remained the same. Another criticism regarding the tests used in the paper at hand is that the presentation of the studies and their results suggests a significant difference between Unexpected Contents Tasks and Unexpected Transfer Tasks. Contrary to this, Wellman et al. conclude that "these task types are remarkably equivalent"\cite{tom_children_2001}, meaning that Kosinski's research lacks the variety needed to come to decisive conclusions on existence of ToM in AI since false-belief tests only attest for one aspect regarding ToM.

Ullman took Kosinski's original work one step further and created modified versions of the already rewritten tests. For Unexpected Content Tasks for example, the container would be described as being transparent or the label would not be readable to the person in the test description. The model would then fail the task and not be able to infer that the person would be able to see inside the container or have no idea (false or correct) of its contents. While Ullman's work has its own flaws (namely only testing GPT-3.5 and only showing examples of results and not of all conducted experiments) it very strongly suggests that the tests used in Kosinski's research do not prove the existence of ToM and might be too close to the original tests for an LLM to not see the relation between the tests \cite{critics3}.

One important note is, that in order to give his findings more context and make them easily understandable to readers foreign to both ToM and LLMs, Kosinski includes a comparison of the LLMs' performances on false-belief tasks to that of children and their ages. While this information due to varying opinions of research groups on the topic is not definite, a comparison to human age is a very effective way of showing the progression of AI. Future research would benefit from combining results from developmental psychology and LLM testing more often to make the results more accessible to a wider audience.


\subsection{Presentation}
Some minor and rather nitpicky weaknesses of the paper lie in the presentation of the results: The appendix for study 2 regarding the performance of GPT-3.5 on scrambled tests is missing the percentage of the reverse answering pattern, as for a completely scramble test description, the model would have passed the test having answered according to the original test or its reversed version. However, only the percentage for answers of the original test is given. For study 1, the appendix is complete. Apart from that there are some other inconsistencies with data from graphs being described differently in the text or words being mixed up.
Regarding that this paper is not peer-reviewed, these flaws in the presentation of the paper are excusable.

\subsection{Conclusion}
The paper at hand is an exciting first step into probing LLMs for ToM and researching the proposition that, since ToM seems to emerge in accordance with language development and the nonconformity and spontaneous emergence of skills in LLMs, such models are a promising candidate for being the first AI and the first known entity besides humans to possess ToM. The paper seems to be placed at the forefront of this research question having been published around the same time as GPT-4 and spawning a series of consecutive papers and articles, referring to Kosinski's work or even directly replying to it \cite{critics1,critics2,critics3}. Therefore, even though the presented research seems to have been cut off prematurely and without reliable conclusions, the mere notion of LLMs having ToM gained enough attention to result in further research regarding the matter, extensively testing recent LLMs and proving that they do in fact not possess ToM, yet \cite{critics1,critics3}.