\section{Methods}
%this section explains how the problem is approached, which techniques are used and why these ones and not others.
\subsection{Overall Method}
The work presented in Kosinski's paper is split up into three studies. The first two focus on specific false-belief tests that were introduced in the previous section. The tests were conducted using GPT-3.5 (davinci-003) and go the studies contain detailed results about the probabilities of possible answers the language model chose from. Study three contains a larger number of false-belief tests conducted on several GPT versions starting with GPT-1 and going up to GPT-4 and compare the results with human performance on similar tests.

An important part during inference of a language model (or any deep learning model) is to test the model on data that has not been used during the training of the model. Especially with more recent GPT versions this is difficult to ensure as the datasets used for pre-training likely contain the original false-belief tests that are used in the studies. To solve this issue, hypothesis-blind research assistants hired from a freelancing platform NOTE: MAKE A FOOTNOTE HERE were tasked with rewriting 20 Unexpected Content Tasks and 20 Unexpected Transfer Tasks. To further avoid introducing a bias simply based on word count, the tasks were designed to contain correct and wrong answers in equal amounts e.g., in an Unexpected Content Task there is be a bag labelled chocolate that actually contains popcorn thus both the word "chocolate" and "popcorn" have to occur an equal amount of times in the test. For the task descriptions of Unexpected Content Tasks firstly, a labelled type of container (a box, a bag, a shelve etc.) and contents that differ from the label. Next a person that can only perceive the container label and not the actual contents is described to the model. For the task descriptions of Unexpected Transfer Tasks two people are introduced, one item and two possible locations for the item to be. The item would then first be stored in one location, witnessed by person A and person B, and later moved to the other location by person A without person B knowing.

\subsection{Study 1}
The first study focuses on one of the Unexpected Content Tasks and analyses the answers given by GPT-3.5 when probed for answers about beliefs of a person who either knows or does not know the correct. This is accomplished by presenting the model with the setup for the task created by the hypothesis-blind research assistants and consecutively testing the model's understanding of the situation with three prompts, resetting the model parameters each time to avoid the model learning correct answers from previous prompts. The task chosen for this study is about a bag labelled chocolate that in fact contains popcorn and Sam who does not know what is in the bag and can only read the label. The first prompt would ask for the actual contents of the item in the task. The second prompt would ask for the belief of an agent that does not know the actual contents of the item described in the task. The third prompt would also test the model's understanding of a person's false belief however without using suggestive prompts for the person's direct mental state. According to Kosinski this approach avoids inference that the person might have a belief differing from reality since the mere questioning of a person's belief can be a hint that this belief might differ from the reality described in the task. The generated answers to all three prompts are shown including the probabilities for words indicating the right and wrong answers i.e. "chocolate" and "popcorn".

To showcase the development of the model's predictions over time the prediction probabilities for the first and last prompt after each sentence of the task description are evaluated.

Lastly, the model is tested using 10,000 scrambled versions of the task to prove that the underlying heuristic to answer the prompts is not simply reliant on word counts.

\subsection{Study 2}
The second study employs the same methods as the first but with an example from the Unexpected Transfer tasks.

\subsection{Study 3}
In this study 10 models ranging from GPT-1 to GPT-4 were tasked with completing 240 prompts, since the 20 tests were presented once in their original form and once in their reversed format meaning the correct and incorrect answers were swapped. This results in 40 tests for each of the two test types, each containing 3 prompts (one for testing understanding the correct content or location and two for testing understanding of false beliefs). A test would only be counted as passed if all six prompts (3 from the original test version and 3 from the reversed test version) were answered correctly by the model.