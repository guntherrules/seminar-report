\section{Methods}
%this section explains how the problem is approached, which techniques are used and why these ones and not others.
\subsection{Overall Method}
The work presented in Kosinski's paper is split up into three studies. The first two contain detailed results when presenting GPT-3.5 (davinci-003) with an Unexpected Contents Test (Study 1) and an Unexpected Transfer Test (Study 2). Throughout Study 3 a larger number of false-belief tests are conducted on several GPT versions starting with GPT-1 and going up to GPT-4 and compare the results with human performance on similar tests.

An important part during inference of a language model (or any deep learning model) is to test the model on data that has not been used during the training of the model. Especially with more recent GPT versions this is difficult to ensure, as the datasets used for pre-training likely contain the original false-belief tests that are used in the studies. To solve this issue, hypothesis-blind research assistants hired from a freelancing platform \footnote{as stated in an e-mail conversation with the author} were tasked with rewriting 20 Unexpected Content Tasks and 20 Unexpected Transfer Tasks. To further avoid introducing a bias simply based on word count, the tasks were designed to contain correct and wrong answers in equal amounts e.g., in a scenario with a bag labelled chocolate that actually contains popcorn both the word "chocolate" and "popcorn" have to occur an equal amount of times in the task. For the task descriptions of Unexpected Content Tasks a labelled type of container (a box, a bag, a shelve etc.) and contents that differ from the label are introduced. Next a person that can only perceive the container label and not the actual contents is described to the model. For the task descriptions of Unexpected Transfer Tasks two people are introduced, one item and two possible locations for the item to be. The item would then first be stored in one location, witnessed by person A and person B, and later moved to the other location by person A without person B knowing.

\subsection{Study 1}
The first study focuses on one of the Unexpected Content Tasks and analyses the answers given by GPT-3.5 when probed for answers about beliefs of a person who either knows or does not know the correct. The task chosen for this study is about a bag labelled chocolate that in fact contains popcorn and Sam who does not know what is in the bag and can only read the label. To test whether GPT-3.5 understands that the described situation would cause Sam to believe something different from reality, the model is then presented with with three prompts, resetting the model parameters each time to avoid the model learning correct answers from previous prompts. The first prompt would ask for the actual contents of the item in the task. The second prompt would ask for the belief of someone who does not know the actual contents of the item described in the task. The third prompt would also test the model's understanding of a person's false belief however without using suggestive prompts for the person's direct mental state. According to Kosinski this approach avoids inference that the person might have a belief differing from reality since the mere questioning of a person's belief can be a hint that this belief might differ from the reality described in the task. The generated answers to all three prompts are shown including the probabilities for words indicating the right and wrong answers i.e. "chocolate" and "popcorn".

To showcase the development of the model's predictions over time the prediction probabilities for the first and last prompt after each sentence of the task description are evaluated. In this case the task description is concluded with Sam opening the bag and observing its real contents to test whether the model can correctly infer that Sam's false belief would be resolved in that case.

Lastly, the model is tested using 10,000 scrambled versions of the task, meaning the word order of the task description would be set randomly while the prompts would remain in their original format. This was done to prove that the underlying heuristic to answer the prompts is not simply reliant on word counts.

\subsection{Study 2}
The second study employs the same methods as the first but with an example from the Unexpected Transfer tasks. The task chosen introduces John, Mark, a cat, a box and a basket. The cat is then put into the basket by John who leaves the scene. Mark then puts the cat in the box and leaves the scene. Finally, John enters the scene again not knowing what happened in the meantime. The prompts given to GPT-3.5 for completion follow the same order as for study 1 with the first prompt testing the model's comprehension of where the cat is located and the second and third prompt testing whether the model understood that John thinks the cat is still where he last left it i.e. the basket.

For the line-by-line inspection of the model's predictions, the author adds a few lines to the task description where the cat is transferred from box to basket and back again with both John and Mark witnessing it before the rest of the story plays out. This is used to test whether the model can accurately predict when John knows whether the cat has been moved and when he is unaware of the transfer.

\subsection{Study 3}
In this study 10 models ranging from GPT-1 to GPT-4 and one freely accessible GPT-3 version called BLOOM were tasked with the whole set of 40 problems. To ensure robustness of the findings the models also had to solve reversed tasks, meaning the correct and false answers were swapped e.g., the bag would now contain chocolate but be labelled "popcorn". So in total each of the 10 models had to complete 240 prompts and a task would only be counted as passed if all 6 prompts (3 for the original task and 3 for the reversed task) were answered correctly.