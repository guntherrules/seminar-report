\section{Future Work}
%this section discusses limitations of the state of the art and presents ideas for future work
Future work regarding ToM in LLMs might include a more complex combination of few-shot learning and special prompts to enable LLMs to develop ToM, which has already been tested on a small scale by Moghaddam and Honey in April 2023 \cite{future}. They were able to show that LLMs could be fine-tuned using specific prompts and two-shot learning i.e. inputting two ToM tests with the corresponding answers first and then testing the model on a third test scenario and thus improve performance of GPT-3.5 and GPT-4 significantly. An upscaling of this experiment could prove if this is one possibility of infusing LLMs with ToM. This could be useful for applications of LLMs where ToM is strictly necessary e.g., moral reasoning or counselling. Furthermore, Kosinski's research and its surrounding critics suggest that using ToM tests designed for humans are applicable to LLMs as well. Further research might include proper testing of this assumption and lead to the development of more refined ToM test databases specifically for LLMs and other NLP applications. For example, Sap et al. \cite{related} suggest that since ToM is learned through social interaction perhaps a more dialogue-based assortment of tests would lead to better results of LLMs when trained on such data. In conclusion, while current LLMs still seem to be lacking regarding ToM the progression from no understanding of ToM tasks to somewhat of an understanding of ToM tasks leaves the open question whether a fine-tuned model of current standards or a merely pre-trained rendition of further developed LLMs of future research efforts might show definitive signs of ToM. This demands both research in LLMs as well as ToM test databases and will likely prove to be an interesting development in the upcoming years.