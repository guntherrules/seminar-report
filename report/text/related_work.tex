\section{Related Work}
%it summarizes the state of the art. Puts the paper in context.
\subsection{Theory of Mind}
While Theory of Mind and the research related to it in a purely psychological understanding is more of a foundation for the paper in question it is still important to know what the term means and the research it is related to, to grasp the context of the paper at hand.

Theory of Mind encompasses all abilities that infer various mental states based on contextual information. The term Theory of Mind (ToM) gained traction in the late 1970s and has since found firm footing in developmental psychology where the term is used to determine the existence and emergence of ToM in children. ToM is also a key part to enable predictions of behaviour based on an agent's mental state.\cite{theory_of_mind}

The research around ToM in child development plays an important role in Kosinski's work since he uses both well-cited methods and results from this field to first construct ToM tests for LLMs and consecutively compare the LLMs' performances to that of children in different age groups. Therefore, the relevant methods and research results will be shortly surmised in the following. In order to assess a child's ability to differentiate reality from beliefs, false-belief tests were introduced by several researchers \cite{fb_test_places_1,fb_test_places_2,fb_test_contents}. In theses tests children were exposed to situations in which they have to make distinctions between reality and the knowledge or belief of another person about reality and predict someone's behaviour according to it. Kosinskis work is based on two kinds of these tests.

First, the Smarties Task developed by Perner et al. in 1987. In this experiment children around age 4 are shown a tube of "Smarties" (chocolate candy), asked its contents and then shown the real contents which were actually random objects and not candy. They would then be asked the questions what another child who did not see the real contents of the tube would expect to be inside, thus testing the reasoning skills about false beliefs of others. \cite{fb_test_contents}

Second, the Sally-Anne Task first used in a study from 1983 \cite{fb_test_places_1} and then again in 1985 \cite{fb_test_places_2} (this time coining the name Sally-Anne Task) tests the same reasoning ability of attributing false beliefs to others. In this task two agents are introduced (typically using puppets). One agent places an item and leaves the scene while the other agent moves the item before the first agent comes back to look for the item. The children are then asked about the expected behaviour of the first agent who did not witness the displacement of the item. The task is solved correctly when the child predicts that the first agent will search the item in the place where they left it before the second agent moved it, thus proving the child's ability to infer false beliefs of other people. \cite{fb_test_places_1,fb_test_places_2}

In alignment with Kosinski's paper the Smarties Task will be called Unexpected Contents Task and the Sally-Anne task will be called Unexpected Transfer Task as a generalisation and for better understanding of the terms.

As for results in developmental psychology regarding ToM, Kosinski refers to the work of Wellman et al. which agglomerates several studies on ToM (and other attributes) of children mainly aged from 2.5 to 6 years old and interpolates their performance on both Smarties and Sally-Anne Tasks \cite{tom_children_2001}. The results of this paper are used to compare the performance of LLMs on similar tasks to the performance of children in an attempt to assign an age to the level of reasoning LLMs supposedly posses.

Finally, it is important to note that these tests though well-established do not cover the whole landscape of ToM testing that currently exists. There is the Faux-Pas Test for social intelligence \cite{tom_test2} or the Reading-the-Mind-in-the-Eyes Test to test emotional intelligence in adults \cite{tom_test1}. These tests are not taken into account in Kosinski's research.

\subsection{Large Language Models}
The second foundational knowledge related to Kosinski's paper is that about Large Language Models (LLMs). Therefore, in this section some key facts about LLMs and specifically GPT-1 to GPT-4 are presented since these models are used in Kosinski's paper.

LLMs are used in the field of Natural Language Processing (NLP) for text generation, question answering or other text-related tasks that they can be fine-tuned to do. The underlying architecture most frequently used is a Transformer which revolutionised deep learning techniques when it was introduced in 2017 \cite{attention}. Transformers in NLP get a tokenised version of the input text and output probabilities for each word of the vocabulary for the output text generation. The most important part here is that during training the Transformer trains an Attention layer, which can basically learn contextual relations of words.

For LLMs as the name suggests the trainable parameters reach very high numbers (up to several billions). The models are pre-trained on a large text databases and can be fine-tuned on specific tasks to create custom models. Generative Pre-Trained Transformers (GPT) first got published by OpenAI in 2018 with GPT-1 which had a parameter size of approximately 110 million and pre-trained on roughly 7,000 books \cite{gpt1}. Since then the model got developed further and over several iterations and grew both in parameter size and training data. GPT-3 already has 175 billion parameters and was only published two years later \cite{gpt3}.

An interesting characteristic of LLMs compared to smaller language models is that they develop skills as byproducts \cite{llms}. LLMs are mostly trained on completing text prompts and no other abilities are specifically engineered into their architecture, so any other ability like translating text or reading comprehension spontaneously emerge \cite{gpt3}. While smaller models require fine-tuning to solve more specific tasks than simple prompt completion, LLMs outperform other state-of-the-art models \cite{gpt3}.

\subsection{Theory of Mind in Large Language Models}
As for the combination of the topics above: Theory of Mind in Large Language Models, a paper published a few months before Kosinski's take on this topic heavily suggests that LLMs do not possess ToM. The presented experiments consist of tests using GPT-3 both in pre-trained form and few-shot form up to 24. The datasets used are SocialIQa probing emotional and social intelligence \cite{socialiqa} and ToMi QA \cite{tomi} resulting in almost 4,000 questions asked for each model. The results of this method show that GPT-3 at the time has below 60\% accuracy in one-shot applications and reaches 60\% accuracy given 24 training examples before solving the task and thus perform far below humans. \cite{related}

Other papers relating to Kosinski's work will be discussed in later sections of this report as they are more recent than the paper in question, follow up on the research presented and therefore are easier to put into context once the methods and results of Kosinski have been explained in the sections coming up.